# ðŸ¤Ÿ Sign Language Recognition using Deep Learning

This project detects and translates **American Sign Language (ASL)** hand gestures into spoken letters using **real-time video**, a **Convolutional Neural Network (CNN)** model, and **text-to-speech (TTS)**. It supports both **single and two-hand gestures**, including complex signs like the letter **Z**.

---

## ðŸ”§ Features

- ðŸ“· Real-time webcam-based gesture recognition
- ðŸ§  Trained CNN model using ASL image dataset
- ðŸ”¤ Recognizes A-Z and 0-9 (36 classes)
- ðŸ”ˆ Text-to-speech audio output using `pyttsx3`
- âœŒï¸ Two-hand gesture support (e.g., for "Z")
- âš¡ Fast and lightweight prediction loop
- ðŸ§¼ Data preprocessing notebook included

---

## ðŸ“ Project Structure

SignLanguageRecognition/
â”‚
â”œâ”€â”€ model/
â”‚ â””â”€â”€ asl_model.h5 # Trained CNN model
â”œâ”€â”€ sign_language_detect.py # Real-time gesture recognition script
â”œâ”€â”€ data_preprocessing.ipynb # Notebook for dataset loading & training
â”œâ”€â”€ requirements.txt # List of required Python packages
â”œâ”€â”€ sample_images/ # Example hand signs (optional)
â””â”€â”€ README.md # Project info (you're here!)


---

## ðŸš€ How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/VinaySMVS/sign-language-detection.git
cd sign-language-detection

2. Install Requirements
pip install -r requirements.txt

3. Run the Detection Script
python sign_language_detect.py

Your webcam will start, and the recognized ASL letters will be displayed and spoken.

4. Train the Model (Optional)
jupyter notebook data_preprocessing.ipynb

Follow the steps in the notebook to preprocess data and train the model. The final model will be saved as asl_model.h5.
ðŸ§  Model Info
Input size: 64x64 RGB images

Model: Convolutional Neural Network (CNN)

Output: 36 softmax units (26 letters + 10 digits)

Optimizer: Adam

Loss: Categorical Crossentropy

Validation Accuracy: > 99%

ðŸŽ¯ Dataset
You can use any open ASL dataset such as:

Kaggle's ASL Alphabet

Custom images captured using webcam

Each class should have folders of labeled images (A-Z, 0-9).
ðŸ“¦ Requirements
These are listed in requirements.txt, but for reference:

opencv-python
tensorflow
numpy
pyttsx3
mediapipe

Install with:
pip install -r requirements.txt
ðŸ§  Future Enhancements
Add sentence formation using time-sequenced gestures

Upload web demo using Streamlit or Flask

Include regional sign languages (e.g., ISL)

Mobile app integration with TensorFlow Lite

Gesture smoothing to reduce flickering predictions

ðŸ‘¤ Author
Vinay SMVS
ðŸ”— GitHub
ðŸ“§ Email: vcpdeloli@example.com